\begin{savequote}[45mm]
\ascii{Any fool can write code that a computer can understand. Good programmers write code that humans can understand.}
\qauthor{\ascii{- Martin Fowler}}
\end{savequote}

\chapter{系统架构} 
\label{ch:architecture}

\begin{content}

本章将阐述\tf{}的系统架构，并一个简单的例子，讲述图结构的变换过程。最后，通过挖掘会话管理的工作机制，加深理解\tf{}运行时的工作机理。

\end{content}

\section{系统架构}
	
\begin{content}

如\refig{tf-architecture}所示，\tf{}的系统结构以\ascii{C API}为界，将整个系统分为\emph{前端}和\emph{后端}两个子系统\footnote{事实上，后端系统中也存在\ascii{Client}的代码，前端系统是\tf{}对外的编程接口。在后面的章节，将详细地讨论这个问题。}。

\begin{enum}
  \eitem{前端系统：提供编程模型，负责构造计算图；}
  \eitem{后端系统：提供运行时环境，负责执行计算图。} 
\end{enum}

\tf{}的系统设计遵循良好的分层架构，后端系统的设计和实现可以进一步分解为\ascii{4}层。

\begin{enum}
  \eitem{运行时：分别提供本地模式和分布式模式，并共享大部分设计和实现；}
  \eitem{计算层：由各个\ascii{OP}的\ascii{Kernel}实现组成；在运行时，\ascii{Kernel}实现执行\ascii{OP}的具体数学运算；} 
  \eitem{通信层：基于\ascii{gRPC}实现组件间的数据交换，并能够在支持\ascii{IB}网络的节点间实现\ascii{RDMA}通信；}
  \eitem{设备层：计算设备是\ascii{OP}执行的主要载体，\tf{}支持多种异构的计算设备类型。}
\end{enum}

从图操作的角度看待系统行为，\tf{}运行时就是完成计算图的构造、编排、及其运行。

\begin{enum}
  \eitem{表达图：构造计算图，但不执行图；}
  \eitem{编排图：将计算图的节点以最佳的执行方案部署在集群中各个计算设备上执行；} 
  \eitem{运行图：按照拓扑排序执行图中的节点，并启动每个\ascii{OP}的\ascii{Kernel}计算。}   
\end{enum}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/tf-architecture.png}
\caption{TensorFlow系统架构}
 \label{fig:tf-architecture}
\end{figure}

\subsection{Client}

\ascii{Client}是前端系统的主要组成部分，它是一个支持多语言的编程环境。\ascii{Client}基于\ascii{TensorFlow}的编程接口，构造计算图。目前，\ascii{TensorFlow}支持\ascii{Python}和\ascii{C++}的编程接口较为完善，尤其对\ascii{Python}的\ascii{API}支持最为全面。并且，对其他编程语言的\ascii{API}支持日益完善。

此时，\ascii{TensorFlow}并未执行任何的图计算，直至与后台计算引擎建立\ascii{Session}，并以\ascii{Session}为桥梁，建立\ascii{Client}与\ascii{Master}之间的通道，并将\ascii{Protobuf}格式的\ascii{GraphDef}序列化后传递给\ascii{Master}，启动计算图的执行过程。

\subsection{Master}

在分布式的运行时环境中，\ascii{Client}执行\code{Session.run}时，传递整个计算图给后端的\ascii{Master}。此时，计算图是完整的，常称为\emph{\ascii{Full Graph}}。随后，\ascii{Master}根据\code{Session.run}传递给它的\code{fetches, feeds}参数列表，反向遍历\ascii{Full Graph}，并按照依赖关系，对其实施剪枝，最终计算得到最小的依赖子图，常称为\ascii{Client Graph}。

接着，\ascii{Master}负责将\ascii{Client Graph}按照任务的名称分裂(\code{SplitByTask})为多个\ascii{Graph Partition}；其中，每个\ascii{Worker}对应一个\ascii{Graph Partition}。随后，\ascii{Master}将\ascii{Graph Partition}分别注册到相应的\ascii{Worker}上，以便在不同的\ascii{Worker}上并发执行这些\ascii{Graph Partition}。最后，\ascii{Master}将通知所有\ascii{Work}启动相应\ascii{Graph Partition}的执行过程。

其中，\ascii{Work}之间可能存在数据依赖关系，\ascii{Master}并不参与两者之间的数据交换，它们两两之间互相通信，独立地完成交换数据，直至完成所有计算。

\subsection{Worker}

对于每一个任务，\tf{}都将启动一个\ascii{Worker}实例。\ascii{Worker}主要负责如下\ascii{3}个方面的职责：

\begin{enum}
  \eitem{处理来自\ascii{Master}的请求；}
  \eitem{对注册的\ascii{Graph Partition}按照本地计算设备集实施二次分裂(\code{SplitByDevice})，并通知各个计算设备并发执行各个\ascii{Graph Partition}；}
  \eitem{按照拓扑排序算法在某个计算设备上执行本地子图，并调度\ascii{OP}的\ascii{Kernel}实现；} 
  \eitem{协同任务之间的数据通信。}
\end{enum}

首先，\ascii{Worker}收到\ascii{Master}发送过来的图执行命令，此时的计算图相对于\ascii{Worker}是完整的，也称为\ascii{Full Graph}，它对应于\ascii{Master}的一个\ascii{Graph Partition}。随后，\ascii{Worker}根据当前可用的硬件环境，包括\ascii{(GPU/CPU)}资源，按照\ascii{OP}设备的约束规范，再将图分裂\code{(SplitByDevice)}为多个\ascii{Graph Partition}；其中，每个计算设备对应一个\ascii{Graph Partition}。接着，\ascii{Worker}启动所有的\ascii{Graph Partition}的执行。最后，对于每一个计算设备，\ascii{Worker}将按照计算图中节点之间的依赖关系执行拓扑排序算法，并依次调用\ascii{OP}的\ascii{Kernel}实现，完成\ascii{OP}的运算(一种典型的多态实现技术)。

其中，\ascii{Worker}还要负责将\ascii{OP}运算的结果发送到其他的\ascii{Worker}上去，或者接受来自其他\ascii{Worker}发送给它的运算结果，以便实现\ascii{Worker}之间的数据交互。\tf{}特化实现了源设备和目标设备间的\ascii{Send/Recv}。

\begin{enum}
  \eitem{本地\ascii{CPU}与\ascii{GPU}之间，使用\code{cudaMemcpyAsync}实现异步拷贝；}
  \eitem{本地\ascii{GPU}之间，使用端到端的\ascii{DMA}操作，避免主机端\ascii{CPU}的拷贝。} 
\end{enum}

对于任务间的通信，\tf{}支持多种通信协议。

\begin{enum}
  \eitem{\ascii{gRPC over TCP；}}
  \eitem{\ascii{RDMA over Converged Ethernet。}} 
\end{enum}

此外，\tf{}已经初步开始支持\ascii{cuNCCL}库，用于改善多\ascii{GPU}间的通信。

\subsection{Kernel}

\ascii{Kernel}是\ascii{OP}在某种硬件设备的特定实现，它负责执行\ascii{OP}的具体运算。目前，\ascii{TensorFlow}系统中包含\ascii{200}多个标准的\ascii{OP}，包括数值计算，多维数组操作，控制流，状态管理等。

一般每一个\ascii{OP}根据设备类型都会存在一个优化了的\ascii{Kernel}实现。在运行时，运行时根据\ascii{OP}的设备约束规范，及其本地设备的类型，为\ascii{OP}选择特定的\ascii{Kernel}实现，完成该\ascii{OP}的计算。

其中，大多数\ascii{Kernel}基于\code{Eigen::Tensor}实现。\code{Eigen::Tensor}是一个使用\ascii{C++}模板技术，为多核\ascii{CPU/GPU}生成高效的并发代码。但是，\ascii{TensorFlow}也可以灵活地直接使用\ascii{cuDNN, cuNCCL, cuBLAS}实现更高效的\ascii{Kernel}。

此外，\ascii{TensorFlow}实现了矢量化技术，在高吞吐量、以数据为中心的应用需求中，及其移动设备中，实现更高效的推理。如果对于复合\ascii{OP}的子计算过程很难表示，或执行效率低下，\ascii{TensorFlow}甚至支持更高效的\ascii{Kernel}注册，其扩展性表现非常优越。

\end{content}

\section{图控制}

\begin{content}

通过一个最简单的例子，进一步抽丝剥茧，逐渐挖掘出\tf{}计算图的控制与运行机制。

\subsection{组建集群}

如\refig{tf-1ps-1worker}所示。假如存在一个简单的分布式环境：\ascii{1 PS + 1 Worker}，并将其划分为两个任务：

\begin{enum}
  \eitem{\ascii{ps0}: 使用\code{/job:ps/task:0}标记，负责模型参数的存储和更新；}
  \eitem{\ascii{worker0}: \code{/job:worker/task:0}标记，负责模型的训练。} 
\end{enum}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/tf-1ps-1worker.png}
\caption{TensorFlow集群：\ascii{1 PS + 1 Worker}}
 \label{fig:tf-1ps-1worker}
\end{figure}

\subsection{图构造}

如\refig{tf-graph-construction}所示。\ascii{Client}构建了一个简单计算图；首先，将$w$与$x$进行矩阵相乘，再与截距$b$按位相加，最后更新至$s$中。

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/tf-graph-construction.png}
\caption{图构造}}
 \label{fig:tf-graph-construction}
\end{figure}

\subsection{图执行}

如\refig{tf-graph-execution}所示。首先，\ascii{Client}创建一个\code{Session}实例，建立与\ascii{Master}之间的通道；接着，\ascii{Client}通过调用\code{Session.run}将计算图传递给\ascii{Master}。

随后，\ascii{Master}便开始启动一次\ascii{Step}的图计算过程。在执行之前，\ascii{Master}会实施一系列优化技术，例如\emph{公共表达式消除}，\emph{常量折叠}等。最后，\ascii{Master}负责任务之间的协同，执行优化后的计算图。

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/tf-graph-execution.png}
\caption{图执行}}
 \label{fig:tf-graph-execution}
\end{figure}

\subsubsection{图分裂}

如\refig{tf-graph-split-by-task}所示，存在一种合理的图划分算法。\ascii{Master}将模型参数相关的\ascii{OP}划分为一组，并放置在\ascii{ps0}任务上；其他\ascii{OP}划分为另外一组，放置在\ascii{worker0}任务上执行。

\begin{figure}[!htbp]
\centering
\includegraphics[width=1.0\textwidth]{figures/tf-graph-split-by-task.png}
\caption{图分裂：按任务划分}}
 \label{fig:tf-graph-split-by-task}
\end{figure}

\subsubsection{子图注册}

如\refig{tf-register-graph}所示。在图分裂过程中，如果计算图的边跨越节点或设备，\ascii{Master}将该边实施分裂，在两个节点或设备之间插入\ascii{Send}和\ascii{Recv}节点，实现数据的传递。

其中，\code{Send}和\code{Recv}节点也是\ascii{OP}，只不过它们是两个特殊的\ascii{OP}，由内部运行时管理和控制，对用户不可见；并且，它们仅用于数据的通信，并没有任何数据计算的逻辑。

最后，\ascii{Master}通过调用\code{RegisterGraph}接口，将子图注册给相应的\ascii{Worker}上，并由相应的\ascii{Worker}负责执行运算。

\begin{figure}[!htbp]
\centering
\includegraphics[width=1.0\textwidth]{figures/tf-register-graph.png}
\caption{子图注册：插入Send和Recv节点}}
 \label{fig:tf-register-graph}
\end{figure}

\subsubsection{子图运算}

如\refig{tf-run-graph}所示。\ascii{Master}通过调用\code{RunGraph}接口，通知所有\ascii{Worker}执行子图运算。其中，\ascii{Worker}之间可以通过调用\code{RecvTensor}接口，完成数据的交换。

\begin{figure}[!htbp]
\centering
\includegraphics[width=1.0\textwidth]{figures/tf-run-graph.png}
\caption{子图执行}}
 \label{fig:tf-run-graph}
\end{figure}

\end{content}

\section{会话管理}
	
\begin{content}

接下来，通过概述会话的整个生命周期过程，及其与图控制之间的关联关系，进一步揭开运行时的内部运行机制。

\subsection{创建会话}

首先，\ascii{Client}\emph{首次}执行\code{tf.Session.run}时，会将整个图序列化后，通过\ascii{gRPC}发送\code{CreateSessionRequest}消息，将图传递给\ascii{Master}。

随后，\ascii{Master}创建一个\code{MasterSession}实例，并用全局唯一的\code{handle}标识，最终通过\code{CreateSessionResponse}返回给\ascii{Client}。如\refig{tf-create-session-overview}所示。

\begin{figure}[!h]
\centering
\includegraphics[width=0.7\textwidth]{figures/tf-create-session-overview.png}
\caption{创建会话}}
 \label{fig:tf-create-session-overview}
\end{figure}

\subsection{迭代运行}

随后，\ascii{Client}会启动迭代执行的过程，并称每次迭代为一次\ascii{Step}。此时，\ascii{Client}发送\code{RunStepRequest}消息给\ascii{Master}，消息携带\code{handle}标识，用于\ascii{Master}索引相应的\code{MasterSession}实例。如\refig{tf-run-step-overview}所示。

\begin{figure}[!h]
\centering
\includegraphics[width=1.0\textwidth]{figures/tf-run-step-overview.png}
\caption{迭代执行}}
 \label{fig:tf-run-step-overview}
\end{figure}

\subsubsection{注册子图}

\ascii{Master}收到\code{RunStepRequest}消息后，将执行图剪枝，分裂，优化等操作。最终按照任务\ascii{(Task)}，将图划分为多个子图片段\ascii{(Graph Partition)}。随后，\ascii{Master}向各个\ascii{Worker}发送\code{RegisterGraphRequest}消息，将子图片段依次注册到各个\ascii{Worker}节点上。

当\ascii{Worker}收到\code{RegisterGraphRequest}消息后，再次实施分裂操作，最终按照设备\ascii{(Device)}，将图划分为多个子图片段\ascii{(Graph Partition)}。\footnote{在分布式运行时，图分裂经过两级分裂过程。在\ascii{Master}上按照任务分裂，而在\ascii{Worker}按照设备分裂。因此，得到结果都称为子图片段，它们仅存在范围，及其大小的差异。}

当\ascii{Worker}完成子图注册后，通过返回\code{RegisterGraphReponse}消息，并携带\code{graph\_handle}标识。这是因为\ascii{Worker}可以并发注册并运行多个子图，每个子图使用\code{graph\_handle}唯一标识。

\subsubsection{运行子图}

\ascii{Master}完成子图注册后，将广播所有\ascii{Worker}并发执行所有子图。这个过程是通过\ascii{Master}发送\code{RunGraphRequest}消息给\ascii{Worker}完成的。其中，消息中携带\code{(session\_handle, graph\_handle, step\_id)}三元组的标识信息，用于\ascii{Worker}索引相应的子图。

\ascii{Worker}收到消息\code{RunGraphRequest}消息后，\ascii{Worker}根据\code{graph\_handle}索引相应的子图。最终，\ascii{Worker}启动本地所有计算设备并发执行所有子图。其中，每个子图放置在单独的\code{Executor}中执行，\code{Executor}将按照拓扑排序算法完成子图片段的计算。上述算法可以形式化地描述为如下代码。


\begin{leftbar}
  \begin{python}
def run_partitions(rendezvous, executors_and_partitions, inputs, outputs):
  rendezvous.send(inputs)
  for (executor, partition) in executors_and_partitions: 
    executor.run(partition)
  rendezvous.recv(outputs)
  \end{python}
\end{leftbar}

\subsubsection{交换数据}

如果两个设备之间需要交换数据，则通过插入\ascii{Send/Recv}节点完成的。特殊地，如果两个\ascii{Worker}之间需要交换数据，则需要涉及跨进程间的通信。

此时，需要通过接收端主动发送\code{RecvTensorRequest}消息到发送方，再从发送方的信箱里取出对应的\ascii{Tensor}，并通过\code{RecvTensorResponse}返回。如\refig{tf-recv-tensor-overview}所示。

\begin{figure}[!h]
\centering
\includegraphics[width=0.7\textwidth]{figures/tf-recv-tensor-overview.png}
\caption{Worker之间的数据交换}}
 \label{fig:tf-recv-tensor-overview}
\end{figure}

\subsection{关闭会话}

当计算完成后，\ascii{Client}向\ascii{Master}发送\code{CloseSessionReq}消息。\ascii{Master}收到消息后，开始释放\code{MasterSession}所持有的所有资源。如\refig{tf-close-session-overview}所示。

\begin{figure}[!h]
\centering
\includegraphics[width=0.7\textwidth]{figures/tf-close-session-overview.png}
\caption{关闭会话}}
 \label{fig:tf-close-session-overview}
\end{figure}

\end{content}
